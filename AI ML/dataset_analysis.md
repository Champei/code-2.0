## üîç Problem Statement: Dataset Analysis Project

### üéØ Objective

Students will explore and analyze one or more of the following datasets to extract insights, build predictive models (if relevant), and communicate findings clearly. Emphasis is on thorough analysis + reproducibility + presentation.

The datasets available:

1. **[Student Stress Monitoring](https://www.kaggle.com/datasets/mdsultanulislamovi/student-stress-monitoring-datasets)** ‚Äî physiological / survey data about student stress.
2. **[Recipe Ingredients](https://www.kaggle.com/datasets/kaggle/recipe-ingredients-dataset)** ‚Äî data about recipes and their ingredients.
3. **[Student Performance Factors](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)** ‚Äî factors (demographic, behavioral, academic, etc.) that might influence student performance.

You can pick one dataset or combine them (if sensible) to do comparative or integrative analysis.

NOTE: You can also pick your own dataset.

---

### ‚öôÔ∏è Core / Required Tasks

Your notebook should include:

1. **Introduction & Problem Definition**

   * Which dataset(s) you chose & why.
   * What are your analysis questions / hypotheses? (e.g., What drives student stress? Which recipe ingredients are most common? What factors most affect student performance?)

2. **Data Preprocessing**

   * Load data, check structure / schema.
   * Handle missing values, duplicates.
   * Data cleaning (formatting, type conversions, outliers).
   * Feature engineering: e.g. create derived variables, encode categorical features, normalize/scale if needed.

3. **Exploratory Data Analysis (EDA)**

   * Summary statistics: means, variances, distributions.
   * Visualizations: histograms, boxplots, scatterplots, heatmaps (for correlations).
   * Identify patterns, relationships, anomalies.

4. **Modeling / Inference (if applicable)**

   * If dataset permits, build predictive models (e.g., regression, classification) to test hypotheses.
   * Cross-validation, train/test split.
   * Evaluate performance (accuracy, precision, recall, RMSE, etc.).
   * Feature importance: which variables seem to matter most?

5. **Interpretation & Insights**

   * What did you find? Are your hypotheses supported?
   * Are there surprising results? Possible confounders?
   * Limitations of the data / analysis.

6. **Visualization & Presentation**

   * Clean, labelled, legible plots.
   * Use narrative: title, sections, headings so someone reading flows through your logic.
   * Supplement with tables where needed.

7. **Conclusion + Recommendations**

   * Summarize the core insights.
   * Suggest what to do next (further data to gather, other models, interventions, etc.)

8. **Reproducibility / Code Quality**

   * Clean, well-commented code.
   * Version control or at least mention package versions.
   * Efficient, not unnecessarily messy.
   * Use functions or modular code where helpful.

---

### üöÄ Optional / Extension Tasks

* Combine datasets (if applicable) for comparative insights (e.g. compare student factors vs stress).
* Use advanced modeling: decision trees, random forest, boosting, or even simple neural nets.
* Use text analysis if there are textual fields (recipes ingredients).
* Clustering, if you want unsupervised learning.
* Time‚Äêseries analysis if data has temporal dimension.
* Deploy a mini-dashboard or interactive visualisation (e.g., with Plotly or Dash).

---

## üìö Rubric / What ‚ÄúGood‚Äù Looks Like

| Component                      | Weight / Importance | What distinguishes excellent work vs average vs poor                                                                                                                                  |
| ------------------------------ | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Problem Definition & Questions | 10%                 | Excellent: clear, non-trivial questions/hypotheses that can be tested. Average: vague or generic. Poor: no real hypothesis or purpose.                                                |
| Data Cleaning & Preprocessing  | 15%                 | Excellent: handles edge cases, documents assumptions, justifies imputations. Average: basic cleaning but misses some issues. Poor: messy data, errors, missing handling.              |
| EDA & Visualization            | 20%                 | Excellent: insightful plots, reveals non-obvious patterns, good visual design. Average: standard plots but little insight. Poor: minimal or confusing visuals.                        |
| Modeling / Inference           | 20%                 | Excellent: appropriate model choice, good validation, meaningful features. Average: model works but perhaps overfitting or under-explored. Poor: no modeling or inappropriate models. |
| Interpretation & Insight       | 20%                 | Excellent: connects results to real-world meaning, acknowledges limitations. Average: states findings but superficial. Poor: just reports metrics with no insight.                    |
| Presentation & Code Quality    | 15%                 | Excellent: clean notebook, comments, readable, minimal bugs. Average: functional but messy. Poor: disorganized, hard to follow.                                                       |


---